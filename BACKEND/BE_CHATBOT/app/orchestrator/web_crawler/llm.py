from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain_openai import ChatOpenAI
from config.base_config import APP_CONFIG
from factories.chat_factory import create_chat_model
import os

# Load chat config
chat_config = APP_CONFIG.chat_model_config

# Create LLM instance
if not chat_config:
    llm = ChatOpenAI(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=3000
    )
else:
    llm = create_chat_model(chat_config)

# Response generation function
def get_context(context: str, user_question: str) -> str:
    """
    Use context to answer a user's question with a structured prompt.

    Args:
        context (str): Extracted or summarized knowledge source.
        user_question (str): The userâ€™s question.

    Returns:
        str: Response generated by the language model.
    """
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question", "context"],
        template=(
            "You are an AI assistant fluent in both Vietnamese and English.\n"
            "ONLY use the provided context to answer the user's question.\n\n"
            "always return image URLs if any\n\n"
            "Context:\n{context}\n\n"
            "Question:\n{question}"
        )
    )

    # Compose the prompt with the model
    llm_chain: Runnable = QUERY_PROMPT | llm

    # Run and return the response
    response = llm_chain.invoke({"question": user_question, "context": context})
    return response.content if hasattr(response, "content") else str(response)
