{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import cloudscraper\n",
    "import httpx  \n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from markitdown import MarkItDown \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "import asyncio\n",
    "import tempfile\n",
    "import traceback\n",
    "from typing import Optional, Tuple,List\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCINTEL_ENDPOINT = \"<document_intelligence_endpoint>\"\n",
    "BASE_URL = \"https://fptshop.com.vn\"\n",
    "class FPTCrawler:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the FPTCrawler class for fetching and processing tour data.\n",
    "        \n",
    "        Args:\n",
    "            base_url: The base URL for resolving relative URLs.\n",
    "            docintel_endpoint: The endpoint for the MarkItDown service.\n",
    "            output_dir: Directory to save processed markdown files.\n",
    "        \"\"\"\n",
    "        self.base_url = BASE_URL\n",
    "        self.docintel_endpoint = DOCINTEL_ENDPOINT\n",
    "        self.output_dir = \"markdown_dir\"\n",
    "\n",
    "    async def fetch_html(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Fetches HTML content by running both httpx and cloudscraper methods concurrently.\n",
    "        Returns the result from whichever method completes successfully first.\n",
    "        \"\"\"\n",
    "        httpx_task = asyncio.create_task(self.fetch_html_httpx(url))\n",
    "        cloudscraper_task = asyncio.create_task(self.fetch_html_cloudscraper(url))\n",
    "\n",
    "        \n",
    "        pending_tasks = {httpx_task, cloudscraper_task}\n",
    "        html_content = None\n",
    "        while pending_tasks and html_content is None:\n",
    "            done_tasks, pending_tasks = await asyncio.wait(\n",
    "                pending_tasks, \n",
    "                return_when=asyncio.FIRST_COMPLETED\n",
    "            )\n",
    "            \n",
    "            for task in done_tasks:\n",
    "                try:\n",
    "                    result, method = task.result()\n",
    "                    if result is not None:\n",
    "                        html_content = result\n",
    "                        print(f\"[Success] Got content using {method}\")\n",
    "                        for pending_task in pending_tasks:\n",
    "                            pending_task.cancel()\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Error] Task failed: {e}\")\n",
    "        \n",
    "        if html_content is None:\n",
    "            print(\"[Error] Unable to fetch content from URL using any method.\")\n",
    "            \n",
    "        return html_content\n",
    "    \n",
    "    def fetch_raw_html_from_url(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Fetches raw HTML content from a URL using cloudscraper.\n",
    "        Returns HTML string or None if there's an error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scraper = cloudscraper.create_scraper(\n",
    "                browser={ \n",
    "                    'browser': 'chrome',\n",
    "                    'platform': 'windows',\n",
    "                    'mobile': False\n",
    "                }\n",
    "            )\n",
    "            response = scraper.get(url, timeout=3)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"[cloudscraper] Failed to fetch: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def fetch_html_httpx(self, url: str) -> Tuple[Optional[str], str]:\n",
    "        \"\"\"\n",
    "        Fetches HTML content using httpx async client.\n",
    "        Returns tuple of (HTML string or None if there's an error, method name).\n",
    "        \"\"\"\n",
    "        method_name = \"httpx\"\n",
    "        try:\n",
    "            async with httpx.AsyncClient(timeout=3, limits=httpx.Limits(max_connections=5)) as client:\n",
    "                response = await client.get(url)\n",
    "                response.raise_for_status()\n",
    "                return response.text, method_name\n",
    "        except Exception as e:\n",
    "            print(f\"[{method_name}] Failed to fetch: {e}\")\n",
    "            return None, method_name\n",
    "\n",
    "    async def fetch_html_cloudscraper(self, url: str) -> Tuple[Optional[str], str]:\n",
    "        \"\"\"\n",
    "        Fetches HTML content using cloudscraper in an executor to not block the event loop.\n",
    "        Returns tuple of (HTML string or None if there's an error, method name).\n",
    "        \"\"\"\n",
    "        method_name = \"cloudscraper\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            result = await loop.run_in_executor(None, self.fetch_raw_html_from_url, url)\n",
    "            return result, method_name\n",
    "        except Exception as e:\n",
    "            print(f\"[{method_name}] Failed to fetch: {e}\")\n",
    "            return None, method_name\n",
    "\n",
    "    def clean_html_content(self, html_content: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Cleans HTML content by removing headers, navs, footers, etc.,\n",
    "        extracts og:image meta tags and processes tables into Markdown.\n",
    "        Returns (cleaned_html, extracted_images).\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        elements_to_remove = [\n",
    "            \"header\", \"nav\", \".header\", \".header--wrapper\", \".navbar\", \n",
    "            \"footer\", \".footer\", \".footer--container\", \n",
    "            \".navigation\", \".nav-bar\", \".menu\", \".main-menu\",\n",
    "            \".sidebar\", \".ads\", \".advertisement\", \".cookie-notice\", \n",
    "            \".social-media\", \".share-buttons\", \".comments\",\n",
    "            # FPTShop specific selectors\n",
    "            \".logo-wrapper\", \".logo\", \".cart-wrapper\", \".main-categories\",\n",
    "            \"a[href*='gio-hang']\",  # Remove cart link\n",
    "            # Hot-key div with search suggestions\n",
    "            \".hot-key\"\n",
    "        ]\n",
    "        for selector in elements_to_remove:\n",
    "            if selector.startswith('.'):\n",
    "                class_name = selector[1:]\n",
    "                for element in soup.find_all(class_=lambda x: x and class_name in x.split()):\n",
    "                    element.decompose()\n",
    "            else:\n",
    "                for element in soup.find_all(selector):\n",
    "                    element.decompose()\n",
    "\n",
    "        extracted_images = []\n",
    "        seen_image_srcs = set()\n",
    "        for tag in soup.find_all(\"meta\", property=\"og:image\"):\n",
    "            src = tag.get(\"content\", \"\")\n",
    "            if (src.endswith(\".png\") or src.endswith(\".jpg\")) and src not in seen_image_srcs:\n",
    "                full_url = urljoin(self.base_url, src)\n",
    "                seen_image_srcs.add(full_url)\n",
    "                extracted_images.append(f\"![Images]({full_url})\")\n",
    "\n",
    "        # Add extraction for all img tags\n",
    "        for img_tag in soup.find_all(\"img\"):\n",
    "            src = img_tag.get(\"src\", \"\")\n",
    "            if (src.endswith(\".png\") or src.endswith(\".jpg\")) and src not in seen_image_srcs:\n",
    "                full_url = urljoin(self.base_url, src)\n",
    "                seen_image_srcs.add(full_url)\n",
    "                extracted_images.append(f\"![Images]({full_url})\")\n",
    "\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            if a_tag.has_attr('href') and not a_tag['href'].startswith(('http://', 'https://', 'data:', '#', 'javascript:')):\n",
    "                a_tag['href'] = urljoin(self.base_url, a_tag['href'])\n",
    "\n",
    "        return str(soup), extracted_images\n",
    "\n",
    "    def format_markdown_content(self, markdown_text: str, extracted_images: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Formats the markdown content by removing duplicates and formatting.\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: The raw markdown text.\n",
    "            extracted_images: List of extracted image URLs formatted as markdown.\n",
    "            \n",
    "        Returns:\n",
    "            Formatted markdown content.\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        result_lines = []\n",
    "\n",
    "        for line in markdown_text.splitlines():\n",
    "            stripped = line.strip()\n",
    "            if stripped.lower().startswith(\"loading\") or not stripped:\n",
    "                continue\n",
    "            if stripped not in seen:\n",
    "                seen.add(stripped)\n",
    "                result_lines.append(line)\n",
    "\n",
    "        # Add main content\n",
    "        content = \"### CONTENT\\n\" + \"\\n\".join(result_lines)\n",
    "\n",
    "        # Append extracted images\n",
    "        if extracted_images:\n",
    "            content += \"\\n\\n### IMAGE\\n\" + \"\\n\".join(extracted_images)\n",
    "\n",
    "        return content\n",
    "\n",
    "    async def convert_url_to_markdown(self, url: str) -> Optional[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Converts HTML content from URL to Markdown after cleaning the HTML.\n",
    "        Uses concurrent fetching to get the fastest successful response.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to fetch and convert.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (formatted_markdown, url) or None if failed.\n",
    "        \"\"\"\n",
    "        temp_html_file_path = None\n",
    "\n",
    "        try:\n",
    "            html_content = await self.fetch_html(url)\n",
    "            \n",
    "            if html_content is None:\n",
    "                print(\"[Error] Unable to fetch content from URL.\")\n",
    "                return None\n",
    "            \n",
    "            cleaned_html, extracted_images = self.clean_html_content(html_content)\n",
    "            \n",
    "            with tempfile.NamedTemporaryFile(mode='w+', suffix='.html', delete=False, encoding='utf-8') as temp_f:\n",
    "                temp_f.write(cleaned_html)\n",
    "                temp_html_file_path = temp_f.name\n",
    "                print(f\"Saved cleaned HTML to temporary file: {temp_html_file_path}\")\n",
    "\n",
    "            md = MarkItDown(docintel_endpoint=self.docintel_endpoint)\n",
    "            result = md.convert(temp_html_file_path)\n",
    "            \n",
    "            formatted_markdown = self.format_markdown_content(result.markdown, extracted_images)\n",
    "            \n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            safe_filename = re.sub(r'\\W+', '_', url.strip())[:50] + \".md\"\n",
    "            output_path = os.path.join(self.output_dir, safe_filename)\n",
    "\n",
    "            with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(formatted_markdown)\n",
    "\n",
    "            print(f\"Successfully converted to Markdown and saved to {output_path}\")\n",
    "            return formatted_markdown, url\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during conversion: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        finally:\n",
    "            if temp_html_file_path and os.path.exists(temp_html_file_path):\n",
    "                try:\n",
    "                    os.remove(temp_html_file_path)\n",
    "                    print(f\"Deleted temporary file: {temp_html_file_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error when deleting temporary file {temp_html_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict\n",
    "from typing import Annotated, List,Literal\n",
    "\n",
    "class FPTData(BaseModel):\n",
    "    \"\"\"Details of an electronic device including specifications, promotions, and warranty.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(extra='forbid')  \n",
    "\n",
    "    device_name: Annotated[\n",
    "        str,\n",
    "        \"Name of the electronic device. Example: 'iPhone 15'\"\n",
    "    ]\n",
    "\n",
    "    storage: Annotated[\n",
    "        List[str],\n",
    "        \"Storage capacitíe in GB. Example: ['256', '512']\"\n",
    "    ]\n",
    "    battery: Annotated[\n",
    "        str,\n",
    "        \"Information about batter .\"\n",
    "    ]\n",
    "    cpu: Annotated[\n",
    "        str,\n",
    "        \"CPU of the computer/laptop. Example: 'Ryzen 5'\"\n",
    "    ]\n",
    "    card: Annotated[\n",
    "        str,\n",
    "        \"graphic card of the laptop.Example: 'AMD Radeon Graphics'\"\n",
    "    ]\n",
    "    screen: Annotated[\n",
    "        str,\n",
    "        \"Information about screen.Example: '14 inch'\"\n",
    "    ]\n",
    "    sale_price: Annotated[\n",
    "        int,\n",
    "        \"Current discounted price in VND. Digits only, no symbols or separators. Example: 4990000\"\n",
    "    ]\n",
    "    original_price: Annotated[\n",
    "        Optional[int],\n",
    "        \"Original price before discount (if available). Digits only. Example: 6490000\"\n",
    "    ]\n",
    "    discount_percent: Annotated[\n",
    "        Optional[int],\n",
    "        \"Discount percentage without % symbol. Example: 23\"\n",
    "    ]\n",
    "    installment_price: Annotated[\n",
    "        Optional[int],\n",
    "        \"Monthly installment amount in VND, digits only. Example: 972750\"\n",
    "    ]\n",
    "    bonus_points: Annotated[\n",
    "        Optional[int],\n",
    "        \"Promotional bonus points awarded. Digits only. Example: 1247\"\n",
    "    ]\n",
    "    suitable_for: Annotated[\n",
    "        Optional[Literal[\n",
    "            \"students\", \"adults\"\n",
    "        ]],\n",
    "        \"what type of customer is suitable for this device based on sale_price. follow strictly this rule: if sale_price less than 1000000 then suitable_for is 'students' and if sale_price is more than 10000000 then suitable_for is 'adult'\"\n",
    "    ] = None\n",
    "    \n",
    "    colors: Annotated[\n",
    "        List[str],\n",
    "        \"Available colors of the device. Example: ['black', 'blue']\"\n",
    "    ]\n",
    "\n",
    "    sales_perks: Annotated[\n",
    "        str,\n",
    "        \"All detailed text (TRY to find in \\\"Quà tặng và ưu đãi khác\\\" and under \\\"Khuyến mãi được hưởng\\\") regarding gifts, other sales perks, extra offers, interest-free installment plans. Include exact text from the website with full promotional wording.\"\n",
    "    ]\n",
    "\n",
    "    guarantee_program: Annotated[\n",
    "        str,\n",
    "        \"ALL Warranty or guarantee program associated with the device. Look for sections like \\\"Bảo hành mở rộng\\\" containing information about extended warranties and care programs.\"\n",
    "    ]\n",
    "    \n",
    "    payment_perks: Annotated[\n",
    "        str,\n",
    "        \"All detailed text (Try to find in \\\"Khuyến mãi thanh toán\\\") regarding promotions, perks, bonus programs, installment options, and payment-related discounts. Include full promotional terms   \"\n",
    "    ]\n",
    "\n",
    "    image_link: Annotated[\n",
    "        str,\n",
    "        \"ALL URL links to the device's featured image in the ### IMAGE section\"\n",
    "    ]\n",
    "\n",
    "    source: Annotated[\n",
    "        str,\n",
    "        \"Source URL of the product page\"\n",
    "    ] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY =os.getenv(\"QDRANT_API_KEY\")\n",
    "QDRANT_COLLECTION_NAME = os.getenv(\"STORAGE\")\n",
    "def ai_model():\n",
    "    return ChatOpenAI(\n",
    "        openai_api_key=OPENAI_API_KEY,       \n",
    "        model=\"gpt-4o-mini\",     \n",
    "        temperature=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_and_create_collection():\n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "    collection_name = QDRANT_COLLECTION_NAME\n",
    "    vector_size = 1536    \n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "        )\n",
    "        print(f\"Created collection '{collection_name}'\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "    return client, collection_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_payload_schema(client, collection_name):\n",
    "    payload_schema = {\n",
    "        \"device_name\": {\"type\": \"text\"},\n",
    "        \"storage\": {\"type\": \"keyword\"},\n",
    "        \"battery\": {\"type\": \"text\"},\n",
    "        \"colors\": {\"type\": \"keyword\"}, \n",
    "        \"cpu\": {\"type\": \"text\"},\n",
    "        \"card\": {\"type\": \"text\"},\n",
    "        \"screen\": {\"type\": \"text\"},\n",
    "        \"suitable_for\": {\"type\": \"text\"},\n",
    "        \"sales_perks\": {\"type\": \"text\"},\n",
    "        \"payment_perks\": {\"type\": \"text\"},\n",
    "        \"guarantee_program\": {\"type\": \"text\"},\n",
    "        \"source\": {\"type\": \"text\"},\n",
    "        \"image_link\": {\"type\": \"text\"},\n",
    "        \"sale_price\": {\"type\": \"integer\"},          \n",
    "        \"original_price\": {\"type\": \"integer\"},      \n",
    "        \"discount_percent\": {\"type\": \"integer\"},   \n",
    "        \"installment_price\": {\"type\": \"integer\"},  \n",
    "        \"bonus_points\": {\"type\": \"integer\"},        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    for field_name, field_config in payload_schema.items():\n",
    "        try:\n",
    "            client.create_payload_index(\n",
    "                collection_name=collection_name,\n",
    "                field_name=field_name,\n",
    "                field_schema=field_config\n",
    "            )\n",
    "            print(f\"Index created for field: {field_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create index for {field_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extract_prompt = \"\"\"\n",
    "    You are tasked with extracting specific information from the \"## Mô tả sản phẩm\" section of a product description for FPT Shop. Your focus is on the following categories:\n",
    "    **FIND INFORMATION ABOUT**\n",
    "    - Design & Materials\n",
    "    - Performance: RAM, \n",
    "    - Camera & Photography Features\n",
    "    - Video Recording Capabilities\n",
    "    - Performance & Hardware\n",
    "    - Battery & Charging\n",
    "    - AI Features\n",
    "    - Comparison with Other Devices\n",
    "    **YOU MUST**\n",
    "    - Locate and extract all relevant information pertaining to the categories listed above.\n",
    "    - Ensure to keep the images links if it goes with the text, (Preserve the original format)\n",
    "    The text to extract from will be provided in the placeholder {input}.\n",
    "\"\"\"\n",
    "\n",
    "async def description(content: str) -> str:\n",
    "\n",
    "    llm = ai_model()\n",
    "    prompt = ChatPromptTemplate.from_template(text_extract_prompt)\n",
    "    chain = prompt | llm\n",
    "    extraction = chain.invoke({\"input\": content})\n",
    "    return extraction.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_metadata_from_context(context: str, source_url: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts structured metadata from FPT shop product content using an AI model.\n",
    "    \"\"\"\n",
    "    llm = ai_model()\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ((\"system\", \"\"\"You are a specialized extractor for FPT Shop product data. Your job is to extract specific fields from product pages exactly as they appear.\n",
    "        **IMPORTANT**:\n",
    "        \n",
    "        1. For sales_perks: Look for sections labeled \"Quà tặng và ưu đãi khác\" AND \"Khuyến mãi được hưởng\" or \"Chính sách sản phẩm\" - include ALL perks, gifts, offers, installment plans, B2B deals with exact wording.\n",
    "        2. For guarantee_program: Look for ALL warranty information, especially under \"Bảo hành mở rộng\" including extended warranties, care programs, and their prices.\n",
    "        3. For payment_perks: Look for section labeled \"Khuyến mãi thanh toán\" and extract ALL payment-related promotions, discounts, and installment options with full details and conditions.\n",
    "        4. Make sure the 'suitable_for' field CAN NOT be null, based on price , if price less than 1000000 VND, suitble_for = 'students' else 'adults' \n",
    "        RULES\n",
    "        - Extract text EXACTLY as it appears in the source\n",
    "        - If a section is not found, return an empty string for that field\"\"\")),\n",
    "        (\"human\", \"Extract the metadata from the following FPT Shop product page text:\\n\\n{context}\")\n",
    "    ])\n",
    "\n",
    "    llm = llm.with_structured_output(schema=FPTData)\n",
    "    chain = prompt | llm \n",
    "    \n",
    "    try:\n",
    "        result: FPTData = chain.invoke({\"context\": context})\n",
    "        metadata = result.model_dump(mode='json')\n",
    "        time_update = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if source_url:\n",
    "            metadata[\"source\"] = source_url\n",
    "        if time_update:\n",
    "            metadata['time_update'] = time_update\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_data(content: str, source_url: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process tour content by concurrently running summarization and metadata extraction.\n",
    "    \n",
    "    Args:\n",
    "        content: The tour content text\n",
    "        source_url: Optional URL source of the content\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing the summary and metadata dictionary\n",
    "    \"\"\"\n",
    "    summary_task = asyncio.create_task(description(content))\n",
    "    metadata_task = asyncio.create_task(extract_metadata_from_context(content, source_url))\n",
    "    \n",
    "    summary, metadata = await asyncio.gather(summary_task, metadata_task)\n",
    "    \n",
    "    return summary, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pipeline(paths):\n",
    "    \"\"\"\n",
    "    Process multiple URLs, convert them to markdown, and add them to Qdrant.\n",
    "    \n",
    "    Args:\n",
    "        paths: List of URLs to process\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each URL\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Initialize the crawler once\n",
    "        tour_data = FPTCrawler()\n",
    "        \n",
    "        # Connect to Qdrant once for all URLs\n",
    "        client, collection_name = connect_and_create_collection()\n",
    "        if not client or not collection_name:\n",
    "            return {\"error\": \"Error in connecting to Qdrant or creating collection.\"}\n",
    "        \n",
    "        # Apply payload schema once\n",
    "        apply_payload_schema(client, collection_name)\n",
    "        \n",
    "        embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Initialize vectorstore once\n",
    "        vectorstore = Qdrant(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            embeddings=embedding_model\n",
    "        )\n",
    "        \n",
    "        for path in paths:\n",
    "            try:\n",
    "                # Convert URL to markdown\n",
    "                result = await tour_data.convert_url_to_markdown(path)\n",
    "                if not result:\n",
    "                    results[path] = \"Error in converting URL to markdown.\"\n",
    "                    continue\n",
    "                    \n",
    "                content, url = result\n",
    "                \n",
    "                # Process the data\n",
    "                description, metadata = await process_data(content, url)\n",
    "                \n",
    "                # Create document\n",
    "                doc = Document(\n",
    "                    page_content=description,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "                # Add the document to the Qdrant vector store\n",
    "                vectorstore.add_documents([doc])\n",
    "                \n",
    "                results[path] = \"Document added to Qdrant successfully.\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred processing {path}: {e}\")\n",
    "                traceback.print_exc()  \n",
    "                results[path] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the pipeline: {e}\")\n",
    "        traceback.print_exc()  \n",
    "        return {\"error\": f\"General pipeline error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'FPT_SHOP' already exists.\n",
      "Index created for field: device_name\n",
      "Index created for field: storage\n",
      "Index created for field: battery\n",
      "Index created for field: colors\n",
      "Index created for field: cpu\n",
      "Index created for field: card\n",
      "Index created for field: screen\n",
      "Index created for field: suitable_for\n",
      "Index created for field: sales_perks\n",
      "Index created for field: payment_perks\n",
      "Index created for field: guarantee_program\n",
      "Index created for field: source\n",
      "Index created for field: image_link\n",
      "Index created for field: sale_price\n",
      "Index created for field: original_price\n",
      "Index created for field: discount_percent\n",
      "Index created for field: installment_price\n",
      "Index created for field: bonus_points\n",
      "[Success] Got content using cloudscraper\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpc8dryy6k.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_dien_thoai_samsung_galaxy_a56.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpc8dryy6k.html\n",
      "[Success] Got content using httpx\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpgf057hn9.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_dien_thoai_samsung_galaxy_a36.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpgf057hn9.html\n",
      "[Success] Got content using cloudscraper\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpcck9apqu.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_dien_thoai_samsung_galaxy_s25.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpcck9apqu.html\n",
      "[Success] Got content using cloudscraper\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmp1mgufzit.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_dien_thoai_iphone_13.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmp1mgufzit.html\n",
      "[Success] Got content using cloudscraper\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpz7itggyp.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_dien_thoai_iphone_15_plus.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpz7itggyp.html\n",
      "[Success] Got content using cloudscraper\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpcr4fkeu8.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_dien_thoai_iphone_14.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpcr4fkeu8.html\n",
      "[Success] Got content using httpx\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpxpe16m38.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_phu_kien_tai_nghe_bluetooth_a.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpxpe16m38.html\n",
      "[Success] Got content using httpx\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmp38yo40wd.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_phu_kien_tai_nghe_bluetooth_n.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmp38yo40wd.html\n",
      "[Success] Got content using cloudscraper\n",
      "Saved cleaned HTML to temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpot1m2t0y.html\n",
      "Successfully converted to Markdown and saved to markdown_dir\\https_fptshop_com_vn_phu_kien_tai_nghe_bluetooth_c.md\n",
      "Deleted temporary file: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpot1m2t0y.html\n",
      "{'https://fptshop.com.vn/dien-thoai/samsung-galaxy-a56': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/dien-thoai/samsung-galaxy-a36': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/dien-thoai/samsung-galaxy-s25-ultra': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/dien-thoai/iphone-13': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/dien-thoai/iphone-15-plus': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/dien-thoai/iphone-14': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/phu-kien/tai-nghe-bluetooth-alpha-works-curve300': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/phu-kien/tai-nghe-bluetooth-nhet-tai-defunc-true-go-slim-den': 'Document added to Qdrant successfully.', 'https://fptshop.com.vn/phu-kien/tai-nghe-bluetooth-choang-dau-co-mic-havit-h628bt': 'Document added to Qdrant successfully.'}\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# List of URLs to process\n",
    "urls = [\n",
    "    \"https://fptshop.com.vn/dien-thoai/samsung-galaxy-a56\",\n",
    "    \"https://fptshop.com.vn/dien-thoai/samsung-galaxy-a36\",\n",
    "    \"https://fptshop.com.vn/dien-thoai/samsung-galaxy-s25-ultra\",\n",
    "    \"https://fptshop.com.vn/dien-thoai/iphone-13\",\n",
    "    \"https://fptshop.com.vn/dien-thoai/iphone-15-plus\",\n",
    "    \"https://fptshop.com.vn/dien-thoai/iphone-14\",\n",
    "    \"https://fptshop.com.vn/phu-kien/tai-nghe-bluetooth-alpha-works-curve300\",\n",
    "    \"https://fptshop.com.vn/phu-kien/tai-nghe-bluetooth-nhet-tai-defunc-true-go-slim-den\",\n",
    "    \"https://fptshop.com.vn/phu-kien/tai-nghe-bluetooth-choang-dau-co-mic-havit-h628bt\",\n",
    "]\n",
    "\n",
    "results = asyncio.run(pipeline(urls))\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
